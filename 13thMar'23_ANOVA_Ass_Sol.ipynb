{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f855d3-5818-4fab-9644-a8916681f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.\n",
    "#Ans-\n",
    "'''ANOVA (Analysis of Variance) is a statistical technique used to compare means between two or more groups. \n",
    "To use ANOVA, certain assumptions must be met for the results to be valid. \n",
    "\n",
    "These assumptions are:\n",
    "1. Independence: The observations within each group must be independent of each other. This means that the values of one observation should not be influenced by or related to the values of other observations.\n",
    "\n",
    "2. Normality: The data within each group should follow a normal distribution. This assumption assumes that the populations from which the samples are taken are normally distributed.\n",
    "\n",
    "3. Homogeneity of Variance: The variance within each group should be approximately equal. This assumption implies that the spread of data within each group should be similar.\n",
    "\n",
    "Violations of these assumptions can impact the validity of ANOVA results. \n",
    "\n",
    "Here are examples of violations for each assumption:\n",
    "1. Independence violation: Independence is violated when the observations within groups are not independent. \n",
    "For example, in a study where the same subjects are measured multiple times, the observations within each subject may be correlated, violating the assumption of independence.\n",
    "\n",
    "2. Normality violation: Normality is violated when the data within groups do not follow a normal distribution. This can occur when the sample size is small or when outliers are present. \n",
    "For example, if the data is heavily skewed or has a heavy-tailed distribution, the assumption of normality may be violated.\n",
    "\n",
    "3. Homogeneity of Variance violation: Homogeneity of variance is violated when the variances within groups are not approximately equal. If the spread of data differs significantly across groups, the assumption of equal variances is violated. \n",
    "For example, if one group has a much larger variance than the other groups, it may impact the validity of ANOVA results.\n",
    "\n",
    "When these assumptions are violated, alternative statistical tests or modifications to ANOVA may be necessary. Non-parametric tests, such as the Kruskal-Wallis test, can be used when the normality assumption is violated. \n",
    "Transformations of the data or the use of robust ANOVA techniques can address violations of homogeneity of variance. \n",
    "However, it is important to note that these alternatives may have their own assumptions and limitations.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed75f1-32f4-4993-a849-9e4bb19d7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What are the three types of ANOVA, and in what situations would each be used?\n",
    "#Ans-\n",
    "'''The three types of ANOVA are:\n",
    "\n",
    "1. One-Way ANOVA: This type of ANOVA is used when you have one categorical independent variable (also known as a factor) with three or more levels, and you want to compare the means of a continuous dependent variable across those levels. \n",
    "For example, if you want to compare the mean test scores of students from different schools (where the schools represent the levels of the independent variable), you would use a One-Way ANOVA.\n",
    "\n",
    "2. Two-Way ANOVA: This type of ANOVA is used when you have two independent variables (factors) and one continuous dependent variable. It allows you to examine the main effects of each independent variable as well as the interaction effect between the two independent variables. \n",
    "For example, if you want to investigate the effects of both gender and treatment type on a patient's recovery time, you would use a Two-Way ANOVA.\n",
    "\n",
    "3. Repeated Measures ANOVA: This type of ANOVA is used when you have a continuous dependent variable measured on the same subjects or units under different conditions or time points. It is specifically designed for within-subject or within-unit designs. \n",
    "Repeated Measures ANOVA allows you to analyze the differences between the means of the repeated measures and examine the interaction effects between the independent variables. \n",
    "For example, if you want to assess the effect of a drug on participants' blood pressure measured at multiple time points, you would use a Repeated Measures ANOVA.\n",
    "\n",
    "Each type of ANOVA is used in specific situations depending on the research design and the nature of the data. \n",
    "It is essential to choose the appropriate ANOVA based on the factors and the dependent variable of interest in order to conduct accurate and meaningful statistical analysis.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95f7cd-c889-4626-aa6e-86a282e44f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "#Ans-\n",
    "'''The partitioning of variance in ANOVA refers to the division of the total variance observed in a data set into different components associated with different sources of variation. \n",
    "Understanding this concept is crucial because it allows us to determine the relative contributions of these sources of variation to the total variance, which helps in interpreting the results of ANOVA and understanding the factors that influence the dependent variable.\n",
    "\n",
    "In ANOVA, the total variance is decomposed into two main components:\n",
    "\n",
    "Between-group variance: This component represents the variability in the dependent variable that is due to differences between the groups or levels of the independent variable. It indicates the extent to which the means of the groups differ from each other. \n",
    "If the between-group variance is large relative to the within-group variance, it suggests that the independent variable has a significant effect on the dependent variable.\n",
    "\n",
    "Within-group variance: This component represents the variability in the dependent variable that is due to individual differences or random fluctuations within each group. It reflects the natural variability or noise in the data that is not accounted for by the independent variable. \n",
    "If the within-group variance is small relative to the between-group variance, it suggests that the differences observed between the groups are unlikely to be due to chance.\n",
    "\n",
    "By understanding the partitioning of variance, researchers can assess the significance of the independent variable's effect on the dependent variable. This is done by comparing the magnitude of the between-group variance to the within-group variance using appropriate statistical tests. \n",
    "If the between-group variance is significantly larger than the within-group variance, it indicates that the groups differ significantly from each other, providing evidence for the effect of the independent variable on the dependent variable.\n",
    "\n",
    "Overall, the partitioning of variance provides a quantitative understanding of the sources of variability in the data and helps in drawing conclusions about the relationships between variables in ANOVA.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c7898e-ac87-4c95-9bb3-c57a212a5347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST: 17.5\n",
      "SSE: 13.0\n",
      "SSR: 4.5\n"
     ]
    }
   ],
   "source": [
    "#Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?\n",
    "#Ans-\n",
    "'''To calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python, you can utilize the statsmodels library. \n",
    "\n",
    "To calculate the SST, we compute the sum of squares of the deviations of the observed values from the mean of the entire dataset. \n",
    "SSE is calculated as the sum of squares of the deviations of the predicted values (obtained from the model) from the mean of the entire dataset. \n",
    "SSR is calculated as the sum of squares of the deviations of the observed values from the predicted values.\n",
    "\n",
    "Here's an example of how you can perform these calculations:\n",
    "'''\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Create a sample dataset\n",
    "data = {'group': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "        'value': [10, 12, 8, 9, 11, 13]}\n",
    "\n",
    "# Convert the dataset to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the one-way ANOVA model\n",
    "model = ols('value ~ group', data=df).fit()\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "ss_total = np.sum((df['value'] - np.mean(df['value']))**2)\n",
    "\n",
    "# Calculate the explained sum of squares (SSE)\n",
    "ss_explained = np.sum((model.fittedvalues - np.mean(df['value']))**2)\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "ss_residual = np.sum((df['value'] - model.fittedvalues)**2)\n",
    "\n",
    "# Print the results\n",
    "print(\"SST:\", ss_total)\n",
    "print(\"SSE:\", ss_explained)\n",
    "print(\"SSR:\", ss_residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec72ca3-aec2-4c0e-bc4e-81a9a78d18a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effects:\n",
      "group1[T.B]   -1.0\n",
      "group1[T.C]   -3.0\n",
      "group2[T.Y]    2.0\n",
      "group2[T.Z]   -2.0\n",
      "dtype: float64\n",
      "Interaction Effect:\n",
      "5.184899015661471e-15\n"
     ]
    }
   ],
   "source": [
    "#Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?\n",
    "#Ans-\n",
    "'''To calculate the main effects and interaction effects in a two-way ANOVA using Python, you can utilize the statsmodels library. \n",
    "Here's an example of how you can perform these calculations:'''\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {'group1': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n",
    "        'group2': ['X', 'Y', 'Z', 'X', 'Y', 'Z', 'X', 'Y', 'Z'],\n",
    "        'value': [10, 12, 8, 9, 11, 13, 7, 10, 9]}\n",
    "\n",
    "# Convert the dataset to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('value ~ group1 + group2 + group1:group2', data=df).fit()\n",
    "\n",
    "# Calculate the main effects\n",
    "main_effects = model.params[['group1[T.B]', 'group1[T.C]', 'group2[T.Y]', 'group2[T.Z]']]\n",
    "\n",
    "# Calculate the interaction effect\n",
    "interaction_effect = model.params['group1[T.B]:group2[T.Y]']\n",
    "\n",
    "# Print the results\n",
    "print(\"Main Effects:\")\n",
    "print(main_effects)\n",
    "print(\"Interaction Effect:\")\n",
    "print(interaction_effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73586f7e-952b-4f47-8ca3-83e00dd1ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In the code above, we first create a sample dataset with two categorical independent variables, 'group1' and 'group2', and a continuous dependent variable 'value'. \n",
    "We then fit the two-way ANOVA model using the ols function from statsmodels.formula.api.\n",
    "\n",
    "To calculate the main effects, we extract the corresponding coefficients from the model's parameters. \n",
    "The main effects represent the differences in the mean values of the dependent variable between the levels of each independent variable, while holding other independent variables constant.\n",
    "\n",
    "To calculate the interaction effect, we extract the coefficient for the interaction term (group1:group2) from the model's parameters. \n",
    "The interaction effect represents the combined effect of both independent variables on the dependent variable, beyond what can be explained by their main effects alone.\n",
    "\n",
    "Finally, we print the calculated main effects and interaction effect. \n",
    "Note that the variable names in the main_effects calculation correspond to the specific levels of each independent variable, denoted by '[T.B]' and '[T.C]' for 'group1' and '[T.Y]' and '[T.Z]' for 'group2' (with '[T.]' indicating the reference level). \n",
    "Adjust the variable names based on your specific dataset and coding scheme.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf4a9c-e7c6-4b24-b612-7fb8e823145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?\n",
    "#Ans-\n",
    "'''In the given scenario, a one-way ANOVA was conducted, resulting in an F-statistic of 5.23 and a p-value of 0.02. To interpret these results and draw conclusions about the differences between the groups, we consider the following:\n",
    "\n",
    "F-Statistic: The F-statistic measures the ratio of variability between the groups (explained variance) to the variability within the groups (unexplained variance). In this case, the F-statistic is 5.23.\n",
    "\n",
    "p-value: The p-value associated with the F-statistic represents the probability of obtaining such an F-statistic or more extreme values if the null hypothesis is true. In this case, the p-value is 0.02.\n",
    "\n",
    "Given these results, we can make the following interpretations and conclusions:\n",
    "\n",
    "There is evidence of a statistically significant difference between the groups: Since the p-value (0.02) is less than the commonly used significance level of 0.05, we reject the null hypothesis. \n",
    "This implies that there is evidence to suggest that there are significant differences between the means of the groups being compared.\n",
    "\n",
    "The groups are not likely to have identical means by chance alone: The obtained F-statistic of 5.23 indicates that the between-group variability (explained variance) is larger than the within-group variability (unexplained variance). \n",
    "This suggests that the observed differences between the groups are unlikely to occur by chance alone and are likely due to real differences in the means.\n",
    "\n",
    "Post hoc tests or further analysis may be necessary: While the ANOVA indicates the presence of statistically significant differences between the groups, it does not provide specific information on which groups differ from each other. \n",
    "To determine the specific group differences, additional post hoc tests (e.g., Tukey's test, Bonferroni test) or pairwise comparisons can be conducted.\n",
    "\n",
    "In summary, based on an F-statistic of 5.23 and a p-value of 0.02 in the one-way ANOVA, we conclude that there are statistically significant differences between the groups being compared. Further analysis can be conducted to identify the specific group differences.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8972c04-055e-42ea-a82c-357ce3a93960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?\n",
    "#Ans-\n",
    "'''Handling missing data in a repeated measures ANOVA requires careful consideration, as it can impact the validity and reliability of the results. Here are some common methods for handling missing data in a repeated measures ANOVA:\n",
    "\n",
    "Complete Case Analysis (Listwise Deletion): This method involves excluding cases with missing data on any of the variables involved in the analysis. It is the simplest approach but can lead to reduced sample size and potential bias if the missing data are not missing completely at random (MCAR).\n",
    "\n",
    "Pairwise Deletion (Available Case Analysis): This approach uses all available data for each specific pairwise comparison. It allows the use of all cases with available data for each comparison, but it can lead to loss of statistical power and may introduce bias if the missing data are not MCAR.\n",
    "\n",
    "Imputation Methods: Imputation involves estimating or replacing missing values with plausible values. Common imputation methods include mean imputation, median imputation, regression imputation, and multiple imputation. \n",
    "These methods aim to preserve sample size and reduce bias, but the accuracy of imputed values depends on the assumptions made during the imputation process.\n",
    "\n",
    "The consequences of using different methods to handle missing data in a repeated measures ANOVA can vary:\n",
    "\n",
    "Bias: If the missing data are not MCAR (i.e., related to the values of the missing data itself or other variables), using complete case analysis or pairwise deletion may introduce bias into the analysis. Imputation methods attempt to reduce bias by providing plausible estimates for missing values.\n",
    "\n",
    "Statistical Power: Complete case analysis and pairwise deletion can result in a reduction in sample size, leading to a decrease in statistical power. Imputation methods that retain the full sample size can help preserve statistical power.\n",
    "\n",
    "Precision and Variance: Different methods for handling missing data can affect the precision of estimates and the variability of the results. Pairwise deletion can result in greater variability, while imputation methods can reduce variability but may introduce additional uncertainty due to the imputed values.\n",
    "\n",
    "Assumptions: Each method for handling missing data makes certain assumptions. Complete case analysis assumes MCAR, while imputation methods assume that the missing data mechanism can be properly modeled. Violation of these assumptions can impact the validity of the results.\n",
    "\n",
    "When handling missing data in a repeated measures ANOVA, it is important to carefully evaluate the nature of the missing data, consider the assumptions of different methods, and choose an approach that is appropriate for the specific dataset and research question. \n",
    "Sensitivity analyses and robustness checks can also be employed to assess the potential impact of different missing data handling methods on the results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393dd483-3fa4-4438-b80b-79c7ccc46184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.\n",
    "#Ans-\n",
    "'''After conducting an ANOVA and finding a significant effect, post-hoc tests are often used to examine specific group differences. Several common post-hoc tests include:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD) test: Tukey's test is used to compare all possible pairwise group means. It controls the overall Type I error rate, making it suitable for situations where you want to examine differences between multiple groups.\n",
    "\n",
    "Bonferroni correction: The Bonferroni correction is a conservative method that adjusts the significance level to account for multiple comparisons. It is commonly used when conducting several pairwise comparisons and helps reduce the risk of Type I errors. The adjusted p-values are compared against the desired significance level.\n",
    "\n",
    "Scheffe's test: Scheffe's test is more conservative than Tukey's test and is suitable for situations where you have unequal sample sizes and want to examine all possible pairwise comparisons. It accounts for multiple comparisons and is robust to violations of assumptions.\n",
    "\n",
    "Dunnett's test: Dunnett's test is used when you have a control group that serves as a reference for comparison against other treatment groups. It is appropriate when you want to determine if the treatment groups differ significantly from the control group.\n",
    "\n",
    "Fisher's Least Significant Difference (LSD) test: Fisher's LSD test is a less conservative post-hoc test used when the assumption of equal variances is met. It compares pairwise group means and is often used in exploratory analyses or when there are a small number of comparisons.\n",
    "\n",
    "Example situation: Let's consider a study comparing the effectiveness of four different teaching methods (A, B, C, and D) on student performance. After conducting an ANOVA, it shows a statistically significant effect of teaching methods on student performance. \n",
    "In this case, a post-hoc test would be necessary to determine which specific pairs of teaching methods differ significantly from each other. Tukey's HSD test or Scheffe's test can be used to conduct pairwise comparisons between all possible combinations of teaching methods. \n",
    "These tests would help identify which teaching methods show significant differences in terms of student performance, providing more detailed insights beyond the overall ANOVA result.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2ef846-532a-4825-b98b-e244cdfe78e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 107.0404663923181\n",
      "p-value: 1.0630852679131706e-22\n"
     ]
    }
   ],
   "source": [
    "'''Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results.'''\n",
    "\n",
    "#Ans-\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Weight loss data for the three diets\n",
    "diet_A = [2.1, 1.9, 2.5, 2.3, 1.8, 1.7, 2.2, 2.0, 2.4, 2.1, 2.2, 2.3, 2.0, 1.9, 1.8, 2.4, 2.5, 2.2, 2.1, 2.3, 1.7, 2.1, 2.2, 2.3, 2.4, 1.9]\n",
    "diet_B = [1.5, 1.3, 1.7, 1.8, 1.9, 1.6, 1.4, 1.2, 1.7, 1.8, 1.6, 1.5, 1.3, 1.7, 1.6, 1.4, 1.5, 1.7, 1.6, 1.3, 1.5, 1.8, 1.6, 1.4, 1.5, 1.7]\n",
    "diet_C = [1.2, 1.3, 1.1, 1.5, 1.4, 1.2, 1.6, 1.3, 1.2, 1.4, 1.5, 1.6, 1.2, 1.4, 1.3, 1.5, 1.4, 1.2, 1.3, 1.5, 1.6, 1.3, 1.5, 1.4, 1.2, 1.6]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314695a-95b5-4a29-a2e6-739b22d5ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''n the code above, weight loss data for the three diets (A, B, and C) are provided. The f_oneway function from scipy.stats is used to perform the one-way ANOVA analysis.\n",
    "\n",
    "The F-statistic and p-value are then printed out. The F-statistic measures the ratio of between-group variability to within-group variability, and the p-value represents the probability of obtaining such an F-statistic or more extreme values if the null hypothesis (no difference between the means) is true.\n",
    "\n",
    "Interpreting the results:\n",
    "Based on the obtained F-statistic and p-value, you can interpret the results as follows:\n",
    "\n",
    "The F-statistic is the ratio of between-group variability to within-group variability. A larger F-statistic indicates a larger difference between the group means relative to the variability within each group. The p-value is used to determine the statistical significance of the observed differences.\n",
    "\n",
    "In this case, if the p-value is less than the chosen significance level (e.g., 0.05), you can conclude that there are significant differences between the mean weight loss of the three diets. \n",
    "A smaller p-value indicates stronger evidence against the null hypothesis and suggests that the observed differences in weight loss are unlikely to occur by chance alone.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c67b58-a053-42e0-a916-8fc030e8e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          sum_sq    df         F    PR(>F)\n",
      "C(Program)                 0.392   2.0  0.259717  0.773409\n",
      "C(Experience)              0.003   1.0  0.003975  0.950249\n",
      "C(Program):C(Experience)   0.728   2.0  0.482332  0.623198\n",
      "Residual                  18.112  24.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "'''Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results.'''\n",
    "\n",
    "#Ans-\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {'Program': ['A', 'B', 'C'] * 10,\n",
    "        'Experience': ['Novice', 'Experienced'] * 15,\n",
    "        'Time': [12.3, 11.8, 12.5, 10.9, 11.2, 10.7, 13.1, 12.9, 12.7, 11.5,\n",
    "                 12.4, 12.1, 10.8, 11.3, 11.0, 13.0, 12.6, 12.2, 10.6, 11.7,\n",
    "                 10.9, 11.4, 11.1, 13.2, 12.8, 11.6, 12.0, 10.7, 11.9, 12.6]}\n",
    "\n",
    "# Convert the dataset to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Time ~ C(Program) + C(Experience) + C(Program):C(Experience)', data=df).fit()\n",
    "\n",
    "# Perform ANOVA table calculations\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943cd3e3-4b7b-4314-a781-8d9a607bd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In the code above, a sample dataset is created with three software programs (A, B, and C), and two levels of employee experience (novice and experienced). The ols function from statsmodels.formula.api is used to fit the two-way ANOVA model, including main effects for the software programs and employee experience level, as well as their interaction effect.\n",
    "\n",
    "The anova_lm function from statsmodels.api is then used to calculate the ANOVA table, which includes the F-statistics and p-values for each effect.\n",
    "\n",
    "Interpreting the results:\n",
    "The ANOVA table will provide the F-statistics and p-values for the main effects and the interaction effect. Here's how you can interpret the results:\n",
    "\n",
    "Main effects:\n",
    "The main effect of the software programs (Program) represents the overall difference in the average time to complete the task between the three programs, regardless of employee experience.\n",
    "The main effect of the employee experience level (Experience) represents the overall difference in the average time to complete the task between novice and experienced employees, regardless of the software program used.\n",
    "Interaction effect:\n",
    "The interaction effect between Program and Experience represents whether the effect of one variable (e.g., Program) on the average time to complete the task depends on the levels of the other variable (e.g., Experience).\n",
    "To interpret the results, look at the p-values associated with each effect. If a p-value is less than the chosen significance level (e.g., 0.05), you can conclude that there is a statistically significant effect.\n",
    "\n",
    "For example, if the p-value for the interaction effect is less than 0.05, it suggests that there is a significant interaction between the software programs and employee experience level, indicating that the effect of the software programs on the average time to complete the task depends on the level of employee experience.\n",
    "\n",
    "Similarly, if the p-values for the main effects are less than 0.05, it indicates that there are significant differences in the average time to complete the task between the software programs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf58fded-b062-4f73-9f80-a4bcbf6cb744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "t-statistic: -3.3458700850103\n",
      "p-value: 0.001856534104382322\n",
      "\n",
      "Post-hoc test results (Tukey's HSD):\n",
      "Tukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\n",
      "Comparison  Statistic  p-value  Lower CI  Upper CI\n",
      " (0 - 1)     83.650     0.000    82.005    85.295\n",
      " (1 - 0)    -83.650     0.000   -85.295   -82.005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other.'''\n",
    "\n",
    "#Ans-\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Test scores for the control group (traditional teaching method)\n",
    "control_scores = [78, 82, 85, 73, 89, 91, 76, 80, 79, 81, 86, 83, 87, 75, 88, 84, 80, 77, 79, 81]\n",
    "\n",
    "# Test scores for the experimental group (new teaching method)\n",
    "experimental_scores = [85, 89, 90, 78, 92, 94, 81, 85, 87, 88, 91, 86, 89, 80, 93, 88, 84, 82, 84, 86]\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print the results of the t-test\n",
    "print(\"Two-sample t-test results:\")\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Perform post-hoc test (if results are significant)\n",
    "if p_value < 0.05:\n",
    "    posthoc_tukey = stats.tukey_hsd(np.concatenate((control_scores, experimental_scores)),\n",
    "                                   np.concatenate(([0] * len(control_scores), [1] * len(experimental_scores))))\n",
    "    print(\"\\nPost-hoc test results (Tukey's HSD):\")\n",
    "    print(posthoc_tukey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39357469-579e-4d5a-b077-d08a40129ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In summary, the t-test results suggest that the new teaching method has a significant impact on test scores compared to the traditional teaching method. The post-hoc test using Tukey's HSD indicates that the experimental group (new teaching method) has significantly lower test scores than the control group (traditional teaching method).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3efff017-9be7-4b9f-a559-77c78264e151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated Measures ANOVA results:\n",
      "          sum_sq    df         F    PR(>F)\n",
      "Store        2.4   2.0  0.312741  0.734053\n",
      "Residual   103.6  27.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "'''Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. \n",
    "They randomly select 30 days and record the sales for each store on those days. \n",
    "Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. \n",
    "If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other.'''\n",
    "\n",
    "#Ans-\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a dataframe with sales data\n",
    "data = pd.DataFrame({\n",
    "    'Store': ['A', 'B', 'C'] * 10,  # Repeated store labels\n",
    "    'Sales': [12, 15, 10, 11, 14, 13, 9, 11, 12, 15, 10, 11, 14, 13, 9, 11, 12, 15, 10, 11, 14, 13, 9, 11,\n",
    "              12, 15, 10, 11, 14, 13]  # Sales data for 30 days for each store\n",
    "})\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "rm_anova = ols('Sales ~ Store', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(rm_anova, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(\"Repeated Measures ANOVA results:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Perform post-hoc test (if results are significant)\n",
    "if anova_table['PR(>F)'][0] < 0.05:\n",
    "    posthoc = sm.stats.multicomp.pairwise_tukeyhsd(data['Sales'], data['Store'])\n",
    "    print(\"\\nPost-hoc test results (Tukey's HSD):\")\n",
    "    print(posthoc.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d481c-82ed-4cb4-8ebe-2aadfa90f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The repeated measures ANOVA results you provided show the following:\n",
    "\n",
    "Store: The sum of squares (SS) for the store variable is 2.4, with 2 degrees of freedom (df). The F-statistic is 0.312741, and the p-value (PR(>F)) is 0.734053.\n",
    "\n",
    "Residual: The SS for the residual (error) term is 103.6, with 27 degrees of freedom.\n",
    "\n",
    "The results indicate that the main effect of store on sales is not statistically significant. This means that the differences observed in the average daily sales between the three retail stores (Store A, Store B, and Store C) could be due to random variability or other factors not accounted for in the analysis.\n",
    "\n",
    "In more detail:\n",
    "\n",
    "The sum of squares (SS) represents the variability explained by the store variable (Store SS) and the unexplained variability or residual (Residual SS).\n",
    "The degrees of freedom (df) represent the number of independent pieces of information available for estimation.\n",
    "The F-statistic is the ratio of the mean squares (MS) between the store and residual terms, calculated as Store MS / Residual MS.\n",
    "The p-value (PR(>F)) associated with the F-statistic represents the probability of obtaining an F-value as extreme as or more extreme than the observed F-value, assuming the null hypothesis (no effect of store) is true. In this case, the p-value is 0.734053, which is greater than the significance level of 0.05 commonly used. \n",
    "Therefore, we do not have enough evidence to reject the null hypothesis and conclude that there is a significant difference in average daily sales between the three stores.\n",
    "The high p-value indicates that the observed differences in sales could be due to chance or other factors that are not related to the store variable. The large residual sum of squares (103.6) compared to the store sum of squares (2.4) suggests that a substantial amount of variability remains unexplained after considering the store factor.\n",
    "\n",
    "In summary, based on the provided ANOVA results, there is no evidence of a significant difference in average daily sales between the three retail stores.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
